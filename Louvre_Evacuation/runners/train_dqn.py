#!/usr/bin/env python3
# python CA-dqn1/Louvre_Evacuation/runners/train_dqn.py
# -*- coding: utf-8 -*-
"""
DQNç–æ•£ç³»ç»Ÿè®­ç»ƒè„šæœ¬
"""

import sys
import os

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ï¼Œä»¥è§£å†³æ¨¡å—å¯¼å…¥é—®é¢˜
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import yaml
import torch
import numpy as np
from Louvre_Evacuation.agents.dqn_agent import DQNAgent
from Louvre_Evacuation.envs.evacuation_env import EvacuationEnv
from Louvre_Evacuation.utils.visualization import PerformanceRecorder, visualize_trajectories
from Louvre_Evacuation.utils.reward_visualizer import RewardTracker

def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def train_dqn():
    """è®­ç»ƒDQNæ™ºèƒ½ä½“"""
    print("å¼€å§‹DQNç–æ•£ç³»ç»Ÿè®­ç»ƒ...")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
    
    # --- æœ€ç»ˆè·¯å¾„ä¿®å¤ï¼šconfigsç›®å½•ä¸Louvre_Evacuationæ¨¡å—åŒçº§ ---
    config_path = os.path.join(project_root, 'configs', 'dqn.yaml')
    config_path = os.path.normpath(config_path)
    
    # åŠ è½½é…ç½®
    config = load_config(config_path)
    
    # åˆ›å»ºä¿å­˜ç›®å½• - åŸºäºé¡¹ç›®æ ¹ç›®å½•æ„å»ºè·¯å¾„
    save_dir = os.path.join(project_root, config.get('save_path', 'dqn_results'))
    os.makedirs(save_dir, exist_ok=True)
    
    # åˆ›å»ºå¥–åŠ±è·Ÿè¸ªå™¨
    reward_tracker = RewardTracker(save_dir=os.path.join(save_dir, 'reward_logs'))
    
    # åˆ›å»ºç¯å¢ƒ
    env_config = config['env']
    env = EvacuationEnv(
        width=env_config['width'],
        height=env_config['height'],
        fire_zones=env_config['fire_zones'],
        exit_location=env_config['exit_location'],
        num_people=env_config['num_people']
    )
    
    print(f"ç¯å¢ƒåˆ›å»ºæˆåŠŸ:")
    print(f"  - åœ°å›¾å°ºå¯¸: {env.width}Ã—{env.height}")
    print(f"  - äººå‘˜æ•°é‡: {env.num_people}")
    print(f"  - å‡ºå£ä½ç½®: {env.exit_location}")
    print(f"  - çŠ¶æ€ç©ºé—´: {env.state_size}")
    print(f"  - åŠ¨ä½œç©ºé—´: {env.action_size}")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    agent_config = config['agent']
    agent = DQNAgent(env.state_size, env.action_size, device, agent_config)
    
    print(f"\næ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ:")
    print(f"  - ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"  - ç½‘ç»œå‚æ•°: {sum(p.numel() for p in agent.q_network.parameters()):,}")
    print(f"  - å­¦ä¹ ç‡: {agent_config['learning_rate']}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {agent_config['batch_size']}")
    
    # è®­ç»ƒå‚æ•°
    episodes = config['episodes']
    update_target_freq = config.get('update_target_freq', 50)
    
    # æ€§èƒ½è®°å½•å™¨
    performance_recorder = PerformanceRecorder()
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nå¼€å§‹è®­ç»ƒ {episodes} ä¸ªå›åˆ...")
    print("-" * 50)
    
    best_reward = float('-inf')
    recent_rewards = []
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        steps = 0
        
        while steps < env.max_steps:
            # å¯ç”¨æ¢ç´¢
            action = agent.act(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ - ä¿®æ­£ä»¥æ¥æ”¶4ä¸ªè¿”å›å€¼
            next_state, reward, done, info = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            agent.remember(state, action, reward, next_state, done)
            
            # è®°å½•å¥–åŠ±
            total_reward += reward
            reward_tracker.record_step(reward)
            
            # æ›´æ–°çŠ¶æ€
            state = next_state
            steps += 1
            
            # è®­ç»ƒæ™ºèƒ½ä½“
            if len(agent.memory) > agent.batch_size:
                agent.learn()
            
            if done:
                break
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if episode % update_target_freq == 0:
            agent.update_target_network()
        
        # è·å–æ€§èƒ½æŒ‡æ ‡
        metrics = env.get_performance_metrics()
        evacuation_rate = metrics['evacuation_rate']
        death_rate = metrics['death_rate']
        
        # è®°å½•å›åˆæ•°æ®
        reward_tracker.record_episode(
            episode=episode,
            total_reward=total_reward,
            steps=steps,
            evacuation_rate=evacuation_rate,
            death_rate=death_rate
        )
        
        performance_recorder.record_episode(env, episode, total_reward)
        
        # æ›´æ–°æœ€ä½³å¥–åŠ±
        if total_reward > best_reward:
            best_reward = total_reward
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            agent.save(os.path.join(save_dir, 'best_model.pth'))
        
        # è®°å½•æœ€è¿‘å¥–åŠ±
        recent_rewards.append(total_reward)
        if len(recent_rewards) > 100:
            recent_rewards.pop(0)
        
        # ç®€åŒ–æ‰“å°è¿›åº¦ - æ¯50å›åˆæ‰“å°ä¸€æ¬¡
        if episode % 50 == 0 or episode == episodes - 1:
            avg_recent_reward = np.mean(recent_rewards)
            print(f"Episode {episode:4d}: "
                  f"Reward={total_reward:7.2f}, "
                  f"Avg100={avg_recent_reward:7.2f}, "
                  f"Steps={steps:3d}, "
                  f"Evac={evacuation_rate:.2%}, "
                  f"Death={death_rate:.2%}, "
                  f"Îµ={agent.epsilon:.4f}")
    
    # è®­ç»ƒå®Œæˆ
    print("\n" + "="*50)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*50)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save(os.path.join(save_dir, 'dqn_model.pth'))
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {os.path.join(save_dir, 'dqn_model.pth')}")
    
    # ä¿å­˜å¥–åŠ±æ•°æ®
    reward_tracker.save_data()
    
    # ç”Ÿæˆæœ€ç»ˆçš„å¥–åŠ±åˆ†æå›¾
    try:
        print("\nç”Ÿæˆæœ€ç»ˆå¥–åŠ±åˆ†æå›¾...")
        
        # åŸºç¡€å¥–åŠ±æ›²çº¿
        reward_tracker.plot_reward_curves(
            save_path=os.path.join(save_dir, 'final_reward_curves.png'),
            show=False
        )
        
        # è¯¦ç»†åˆ†æå›¾
        reward_tracker.plot_detailed_analysis(
            save_path=os.path.join(save_dir, 'detailed_analysis.png'),
            show=False
        )
        
        print("å¥–åŠ±åˆ†æå›¾ç”Ÿæˆå®Œæˆï¼")
        
    except Exception as e:
        print(f"ç”Ÿæˆå¥–åŠ±åˆ†æå›¾å¤±è´¥: {e}")
    
    # æœ€ç»ˆç»Ÿè®¡
    print("\n=== æœ€ç»ˆè®­ç»ƒç»Ÿè®¡ ===")
    reward_tracker.print_statistics()
    
    print(f"\næœ€ä½³å¥–åŠ±: {best_reward:.2f}")
    print(f"æœ€ç»ˆæ¢ç´¢ç‡: {agent.epsilon:.4f}")
    
    # ä¿å­˜æ€§èƒ½æ•°æ®
    df = performance_recorder.get_dataframe()
    df.to_csv(os.path.join(save_dir, 'training_performance.csv'), index=False)
    print(f"æ€§èƒ½æ•°æ®å·²ä¿å­˜åˆ°: {os.path.join(save_dir, 'training_performance.csv')}")
    
    print("\nè®­ç»ƒå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°:", save_dir)
    
    return agent, reward_tracker

if __name__ == "__main__":
    try:
        agent, tracker = train_dqn()
        print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc() 