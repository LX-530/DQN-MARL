#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DQNæ¨¡å‹å¯¼å…¥å’Œä½¿ç”¨æŒ‡å—
æ¼”ç¤ºå¦‚ä½•åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†
"""

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
import yaml
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from agents.dqn_agent import DQNAgent
from envs.evacuation_env import EvacuationEnv

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ModelLoader:
    """æ¨¡å‹åŠ è½½å™¨"""
    
    def __init__(self, config_path='configs/dqn.yaml'):
        """åˆå§‹åŒ–æ¨¡å‹åŠ è½½å™¨"""
        self.config_path = config_path
        self.config = self.load_config()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = self.create_environment()
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agent = self.create_agent()
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if os.path.exists(self.config_path):
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        else:
            print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
            # è¿”å›é»˜è®¤é…ç½®
            return {
                'environment': {
                    'width': 36,
                    'height': 30,
                    'num_people': 150,
                    'fire_zones': [[18, 14], [19, 15], [20, 16]],
                    'exit_location': [36, 15]
                },
                'agent': {
                    'learning_rate': 0.001,
                    'gamma': 0.99,
                    'epsilon': 0.0,  # æ¨ç†æ—¶ä¸ä½¿ç”¨æ¢ç´¢
                    'epsilon_min': 0.0,
                    'epsilon_decay': 1.0,
                    'memory_size': 10000,
                    'batch_size': 32,
                    'hidden_size': 256
                }
            }
    
    def create_environment(self):
        """åˆ›å»ºç¯å¢ƒ"""
        env_config = self.config['environment']
        env = EvacuationEnv(
            width=env_config['width'],
            height=env_config['height'],
            num_people=env_config['num_people'],
            fire_zones=env_config['fire_zones'],
            exit_location=env_config['exit_location']
        )
        print(f"ğŸŒ ç¯å¢ƒåˆ›å»ºæˆåŠŸ: {env.width}Ã—{env.height}, {env.num_people}äºº")
        return env
    
    def create_agent(self):
        """åˆ›å»ºæ™ºèƒ½ä½“"""
        agent_config = self.config['agent']
        agent = DQNAgent(
            state_size=self.env.state_size,
            action_size=self.env.action_size,
            device=self.device,
            config=agent_config
        )
        print(f"ğŸ¤– æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ: çŠ¶æ€ç©ºé—´{self.env.state_size}, åŠ¨ä½œç©ºé—´{self.env.action_size}")
        return agent
    
    def load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False
        
        try:
            # åŠ è½½æ¨¡å‹
            self.agent.load(model_path)
            self.agent.epsilon = 0.0  # æ¨ç†æ—¶ä¸ä½¿ç”¨æ¢ç´¢
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            return True
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def run_single_episode(self, visualize=True):
        """è¿è¡Œå•ä¸ªå›åˆ"""
        print(f"\nğŸƒ å¼€å§‹è¿è¡Œå•ä¸ªå›åˆ...")
        
        # é‡ç½®ç¯å¢ƒ
        state = self.env.reset()
        total_reward = 0
        step_count = 0
        done = False
        
        # è®°å½•è½¨è¿¹
        trajectory = []
        
        while not done:
            # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
            action = self.agent.act(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.env.step(action)
            
            # è®°å½•æ•°æ®
            trajectory.append({
                'step': step_count,
                'state': state,
                'action': action,
                'reward': reward,
                'next_state': next_state,
                'info': info
            })
            
            total_reward += reward
            state = next_state
            step_count += 1
            
            # æ‰“å°è¿›åº¦
            if step_count % 10 == 0:
                evacuation_rate = info.get('evacuation_rate', 0.0)
                death_rate = info.get('death_rate', 0.0)
                print(f"  æ­¥éª¤ {step_count}: å¥–åŠ±={reward:.2f}, ç–æ•£ç‡={evacuation_rate:.1%}, æ­»äº¡ç‡={death_rate:.1%}")
        
        # æœ€ç»ˆç»“æœ
        final_info = trajectory[-1]['info']
        evacuation_rate = final_info.get('evacuation_rate', 0.0)
        death_rate = final_info.get('death_rate', 0.0)
        
        print(f"\nğŸ“Š å›åˆç»“æŸ:")
        print(f"  æ€»æ­¥æ•°: {step_count}")
        print(f"  æ€»å¥–åŠ±: {total_reward:.2f}")
        print(f"  ç–æ•£ç‡: {evacuation_rate:.1%}")
        print(f"  æ­»äº¡ç‡: {death_rate:.1%}")
        
        if visualize:
            self.visualize_episode(trajectory)
        
        return trajectory, total_reward, evacuation_rate, death_rate
    
    def run_multiple_episodes(self, num_episodes=10):
        """è¿è¡Œå¤šä¸ªå›åˆè¿›è¡Œè¯„ä¼°"""
        print(f"\nğŸ¯ å¼€å§‹è¯„ä¼°æ¨¡å‹ ({num_episodes} å›åˆ)...")
        
        results = {
            'rewards': [],
            'evacuation_rates': [],
            'death_rates': [],
            'steps': []
        }
        
        for episode in range(num_episodes):
            print(f"\n--- å›åˆ {episode + 1}/{num_episodes} ---")
            
            trajectory, reward, evacuation_rate, death_rate = self.run_single_episode(visualize=False)
            
            results['rewards'].append(reward)
            results['evacuation_rates'].append(evacuation_rate)
            results['death_rates'].append(death_rate)
            results['steps'].append(len(trajectory))
            
            print(f"å›åˆ {episode + 1}: å¥–åŠ±={reward:.2f}, ç–æ•£ç‡={evacuation_rate:.1%}, æ­»äº¡ç‡={death_rate:.1%}")
        
        # ç»Ÿè®¡ç»“æœ
        self.print_evaluation_results(results)
        self.plot_evaluation_results(results)
        
        return results
    
    def print_evaluation_results(self, results):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print(f"\n" + "="*50)
        print(f"ğŸ“ˆ è¯„ä¼°ç»“æœç»Ÿè®¡:")
        print(f"="*50)
        
        rewards = results['rewards']
        evacuation_rates = results['evacuation_rates']
        death_rates = results['death_rates']
        steps = results['steps']
        
        print(f"ğŸ¯ å¥–åŠ±ç»Ÿè®¡:")
        print(f"  å¹³å‡å¥–åŠ±: {np.mean(rewards):.2f} Â± {np.std(rewards):.2f}")
        print(f"  æœ€é«˜å¥–åŠ±: {np.max(rewards):.2f}")
        print(f"  æœ€ä½å¥–åŠ±: {np.min(rewards):.2f}")
        
        print(f"\nğŸ‘¥ ç–æ•£ç»Ÿè®¡:")
        print(f"  å¹³å‡ç–æ•£ç‡: {np.mean(evacuation_rates):.1%} Â± {np.std(evacuation_rates):.1%}")
        print(f"  æœ€é«˜ç–æ•£ç‡: {np.max(evacuation_rates):.1%}")
        print(f"  æœ€ä½ç–æ•£ç‡: {np.min(evacuation_rates):.1%}")
        
        print(f"\nğŸ’€ æ­»äº¡ç»Ÿè®¡:")
        print(f"  å¹³å‡æ­»äº¡ç‡: {np.mean(death_rates):.1%} Â± {np.std(death_rates):.1%}")
        print(f"  æœ€é«˜æ­»äº¡ç‡: {np.max(death_rates):.1%}")
        print(f"  æœ€ä½æ­»äº¡ç‡: {np.min(death_rates):.1%}")
        
        print(f"\nâ±ï¸  æ­¥æ•°ç»Ÿè®¡:")
        print(f"  å¹³å‡æ­¥æ•°: {np.mean(steps):.1f} Â± {np.std(steps):.1f}")
        print(f"  æœ€å¤šæ­¥æ•°: {np.max(steps)}")
        print(f"  æœ€å°‘æ­¥æ•°: {np.min(steps)}")
    
    def plot_evaluation_results(self, results):
        """ç»˜åˆ¶è¯„ä¼°ç»“æœ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # å¥–åŠ±åˆ†å¸ƒ
        axes[0, 0].hist(results['rewards'], bins=10, alpha=0.7, color='blue', edgecolor='black')
        axes[0, 0].set_title('å¥–åŠ±åˆ†å¸ƒ')
        axes[0, 0].set_xlabel('å¥–åŠ±å€¼')
        axes[0, 0].set_ylabel('é¢‘æ¬¡')
        axes[0, 0].grid(True, alpha=0.3)
        
        # ç–æ•£ç‡åˆ†å¸ƒ
        axes[0, 1].hist([r*100 for r in results['evacuation_rates']], bins=10, alpha=0.7, color='green', edgecolor='black')
        axes[0, 1].set_title('ç–æ•£ç‡åˆ†å¸ƒ')
        axes[0, 1].set_xlabel('ç–æ•£ç‡ (%)')
        axes[0, 1].set_ylabel('é¢‘æ¬¡')
        axes[0, 1].grid(True, alpha=0.3)
        
        # æ­»äº¡ç‡åˆ†å¸ƒ
        axes[1, 0].hist([r*100 for r in results['death_rates']], bins=10, alpha=0.7, color='red', edgecolor='black')
        axes[1, 0].set_title('æ­»äº¡ç‡åˆ†å¸ƒ')
        axes[1, 0].set_xlabel('æ­»äº¡ç‡ (%)')
        axes[1, 0].set_ylabel('é¢‘æ¬¡')
        axes[1, 0].grid(True, alpha=0.3)
        
        # æ­¥æ•°åˆ†å¸ƒ
        axes[1, 1].hist(results['steps'], bins=10, alpha=0.7, color='orange', edgecolor='black')
        axes[1, 1].set_title('æ­¥æ•°åˆ†å¸ƒ')
        axes[1, 1].set_xlabel('æ­¥æ•°')
        axes[1, 1].set_ylabel('é¢‘æ¬¡')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'model_evaluation_{timestamp}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š è¯„ä¼°ç»“æœå›¾è¡¨å·²ä¿å­˜: {filename}")
        plt.show()
    
    def visualize_episode(self, trajectory):
        """å¯è§†åŒ–å•ä¸ªå›åˆ"""
        print(f"ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # æå–æ•°æ®
        steps = [t['step'] for t in trajectory]
        rewards = [t['reward'] for t in trajectory]
        evacuation_rates = [t['info'].get('evacuation_rate', 0.0) for t in trajectory]
        death_rates = [t['info'].get('death_rate', 0.0) for t in trajectory]
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # å¥–åŠ±æ›²çº¿
        axes[0, 0].plot(steps, rewards, 'b-', linewidth=2)
        axes[0, 0].set_title('å¥–åŠ±å˜åŒ–')
        axes[0, 0].set_xlabel('æ­¥æ•°')
        axes[0, 0].set_ylabel('å¥–åŠ±')
        axes[0, 0].grid(True, alpha=0.3)
        
        # ç–æ•£ç‡æ›²çº¿
        axes[0, 1].plot(steps, [r*100 for r in evacuation_rates], 'g-', linewidth=2)
        axes[0, 1].set_title('ç–æ•£ç‡å˜åŒ–')
        axes[0, 1].set_xlabel('æ­¥æ•°')
        axes[0, 1].set_ylabel('ç–æ•£ç‡ (%)')
        axes[0, 1].grid(True, alpha=0.3)
        
        # æ­»äº¡ç‡æ›²çº¿
        axes[1, 0].plot(steps, [r*100 for r in death_rates], 'r-', linewidth=2)
        axes[1, 0].set_title('æ­»äº¡ç‡å˜åŒ–')
        axes[1, 0].set_xlabel('æ­¥æ•°')
        axes[1, 0].set_ylabel('æ­»äº¡ç‡ (%)')
        axes[1, 0].grid(True, alpha=0.3)
        
        # ç´¯ç§¯å¥–åŠ±
        cumulative_rewards = np.cumsum(rewards)
        axes[1, 1].plot(steps, cumulative_rewards, 'purple', linewidth=2)
        axes[1, 1].set_title('ç´¯ç§¯å¥–åŠ±')
        axes[1, 1].set_xlabel('æ­¥æ•°')
        axes[1, 1].set_ylabel('ç´¯ç§¯å¥–åŠ±')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'episode_visualization_{timestamp}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š å›åˆå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {filename}")
        plt.show()


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("ğŸš€ DQNæ¨¡å‹å¯¼å…¥å’Œä½¿ç”¨æ¼”ç¤º")
    print("="*50)
    
    # 1. åˆ›å»ºæ¨¡å‹åŠ è½½å™¨
    loader = ModelLoader()
    
    # 2. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    # å¯ä»¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹æ–‡ä»¶
    model_paths = [
        'dqn_results/best_evacuation_model.pth', # æœ€ä½³ç–æ•£æ¨¡å‹ 
    ]
    
    # å°è¯•åŠ è½½ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹
    model_loaded = False
    for model_path in model_paths:
        if loader.load_model(model_path):
            model_loaded = True
            break
    
    if not model_loaded:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶!")
        print("è¯·ç¡®ä¿å·²ç»å®Œæˆè®­ç»ƒå¹¶ç”Ÿæˆäº†æ¨¡å‹æ–‡ä»¶ã€‚")
        return
    
    # 3. é€‰æ‹©è¿è¡Œæ¨¡å¼
    print(f"\nğŸ® é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. è¿è¡Œå•ä¸ªå›åˆ (è¯¦ç»†å¯è§†åŒ–)")
    print("2. è¿è¡Œå¤šä¸ªå›åˆ (æ€§èƒ½è¯„ä¼°)")
    print("3. ä¸¤è€…éƒ½è¿è¡Œ")
    
    try:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
        
        if choice == '1':
            # è¿è¡Œå•ä¸ªå›åˆ
            loader.run_single_episode(visualize=True)
            
        elif choice == '2':
            # è¿è¡Œå¤šä¸ªå›åˆ
            num_episodes = int(input("è¯·è¾“å…¥è¯„ä¼°å›åˆæ•° (é»˜è®¤10): ") or "10")
            loader.run_multiple_episodes(num_episodes)
            
        elif choice == '3':
            # ä¸¤è€…éƒ½è¿è¡Œ
            print("\nğŸ¯ é¦–å…ˆè¿è¡Œå•ä¸ªå›åˆ...")
            loader.run_single_episode(visualize=True)
            
            print("\nğŸ¯ ç„¶åè¿è¡Œå¤šä¸ªå›åˆè¯„ä¼°...")
            num_episodes = int(input("è¯·è¾“å…¥è¯„ä¼°å›åˆæ•° (é»˜è®¤5): ") or "5")
            loader.run_multiple_episodes(num_episodes)
            
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¿è¡Œå•ä¸ªå›åˆ...")
            loader.run_single_episode(visualize=True)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
    
    print(f"\nâœ… ç¨‹åºè¿è¡Œå®Œæˆ!")


if __name__ == "__main__":
    main() 